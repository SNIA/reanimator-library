==== MINUTES - TRACE2MODEL - IBM COLLABORATION ====

** 24 June 2011 **

This is an initial proposal on the way we can collaborate with IBM on
trace2model project.

Currently, Filebench supports two basic execution abstractions: processes and
threads. To support virtualization emulation, we propose to add two more
abstractions: VMs - virtual machines and HPVs - hyperviser. At the moment, to
define a workload in Filebench, one would write something like that:

define process process1 {
 define thread1 {
   <description of what this thread does>
 }
 define thread2 {
   <description of what this thread does>
 }
 ...
}

define process process2 {
 define thread1 {
   <description of what this thread does>
 }
 define thread2 {
   <description of what this thread does>
 }
 ...
}
...

In the virtualization case, one would define hypervisor and virtual
machines in addition, for example:

define hpv1 {
 define vm1 {
   define process process1 {
     define thread1 {
       <description of what this thread  does>
     }
     define thread1 {
       <description of what this thread  does>
     }
     ...
  }
 ...
 define vm2 {
   define process process1 {
     define thread1 {
       <description of what this thread  does>
     }
     define thread1 {
       <description of what this thread  does>
     }
     ...
  }
 ...
}

define hvp2 {
 <vms with processes with treads defined here, as before>
}

So, when Filebench executes Thread1 operations, it transforms them in according
with VMs and HPVs properties. For example, if Thread1 is defined to writes 1MB
to some file, Filebench can split this write request in a series of 128KB
requests if  VM1 is set up to use IDE controller emulator and HPV1 is VMWare ESX
v.3.1.

In order to understand what transformations happen with I/O requests, we need to
analyse traces from multiple levels. And this is where T2M project comes in: it
should help us to analyse traces. We can focus on this part till FAST
deadline.  After that FSL could work on modifying Filebench to support above
mentioned abstractions. Another modification that we would need to add to
Filebench is NFS-client backend. That's what FSL also could do after the FAST
deadline.

In short, the plan we propose is the following:
1) FSL works on trace analyser
2) IBM provides us with traces from different levels
3) FSL processes these traces using the analyser and makes conclusions:
   3a) validates that the analyser works properly
   3b) looks into how requests transform in a virtualized environment
4) FSL sends generated workload models back to IBM so that IBM can
run them and collect new traces, which FSL can compare to the original
traces for the sake of validity check.

That's the work that we cand do for coming FAST. Filebench modifications and
related work can be postponed to Octorber-December.

** 1 July 2011 **

IBM replied that they like the idea a lot. Dean propsed the following list of
initial items for on-coming discussion:

1) Project goals, responsibilities, paper/FAST

2) Method of sharing traces and tools. Can we setup a wiki and a data
repository?

3) For each trace, need to understand the required trace format into which they
must be converted. Are there any existing tools for converting any of the above
traces? We will definitely need a tool to convert the gpfs traces.

4) Need a way to collect traces from the VM's vfs level (should we just use
strace or can we do something with kprobes? I think you had been working on
something for this).

5) Need to discuss how 'synchronized' the different levels of traces need to be.

6) Need to agree on the levels at which each trace will be taken. Depending on
the level, a different trace type might be used. Currently I see the following
levels:
  a) Guest VFS level (see 1) above)
  b) ESX hypervisor. Do we want just vscsistats tables or actual traces
     (which are still missing the latency information)
  c) File Level: Using GPFS tracing to capture the incoming NFS requests
  d) Block Level: Using GPFS tracing (We store data on a single scsi device to
     make things simple) Note that we use GPFS tracing since it makes it simpler
     to get both the file and block level traces in a single output file.

7) Applications and benchmarks to trace.

Vasily suggests to discuss caching affects at I/O layers as well:

8) How do we emulate caching effects at certain I/O stack levels?

** 7 July 2011 **

The summary of the points discussed during the first meeting:

> 1) Project goals, responsibilities, paper/FAST

For the coming FAST deadline the focus of the project is solely on converting
I/O traces to workload models. Traces from different I/O stack layers (including
GPFS layer) will be collected by IBM and used as one of the use cases in the
paper. As a side product we will be able to characterize how the workload
changes as we go down the stack. In the future (after the FAST deadline) it will
allow us to incorporate I/O stack simulation in Filebench.

> 2) Method of sharing traces and tools. Can we setup a wiki and a data
> repository?

FSL will set up a wiki and a trace repository through which we can exchange
traces and models.

> 3) For each trace,  need to understand the required trace format into which they
> must be converted. Are there any existing tools for converting any of the above
> traces? We will definitely need a tool to convert the gpfs traces.

We will use DataSeries format for trace storage and analysis in this project.
Other formats have to be converted to DataSeries. One can 1) either write a tool
that directly converts a foreign format to DataSeries, 2) either write a tool
that converts foreign format to a CSV file, which then is converted to
the DataSeries using csv2ds tool.

Traces come from different layers, e.g., block layer and file system layer.
Field names in trace records are different for different layers.  We
need to agree on the field names. We will stick to the field specifications
defined by SNIA, but we're not limited by them. FSL has looked into the fields
for the block layer traces and will send the specification to IBM soon.  The
decision on file system trace fields will be done a bit later.

> 4) Need a way to collect traces from the VM's vfs level (should we just use
> strace or can we do something with kprobes? I think you had been working on
> something for this).

There are several ways to collect application/VFS level traces:

1) instrument the application (only if we have application's source code)
2) instrument libc (we won't be able to trace mmap()-based accesses)
3) ptrace (slow and can't trace mmap()-based accesses)
4) intercept system calls & page fault handlers or vfs calls:
	a) hardcoded
	b) kprobes
	c) audit() subsystem
	d) use compiler-based approach (talk to Justin about that)
	e) tracefs

Vasily will look into all these possibilities more carefully and then we can
make a decision.

> 5) Need to discuss how 'synchronized' the different levels of traces need to be.

Discrepancy of several seconds among traces does not seem to be a problem in
this project. For now, we assume that it is OK to just start trace collectors
sequentially one after another before the workload is generated. If in the
future it will cause problems - we'll figure something out.

> 6) Need to agree on the levels at which each trace will be taken.

We agreed on the following levels:
  a) Guest VFS level (trace method to be decided)
  b) Guest Block level (blktrace)
  c) ESX hypervisor (vscsistats)
  d) Storage Server File Level (Using GPFS tracing for incoming NFS requests)
  e) Storage Server Block Level (Using GPFS tracing)

> 7) Applications and benchmarks to trace.

In the order of complexity and timing:

a) Filebench micro-workloads
b) Filebench macro-workloads (oltp, fileserver, etc.)
c) Application emulators (web, db, mail stressers)
d) Real-world traces

> 8) Other

Anna noticed that along with trace one needs to collect accuracy parameters. So,
we need to decide on the set of accuracy parameters soon.

IBM has their own scripts to collect GPFS traces. Can we easily add them to the
framework?.. Looks like yes. We will discuss this with Anna via private e-mails.

Renu noticed that often Storage Servers export their data via iSCSI, so we at
least need to mention this. But it might be even better to use iSCSI, not NFS,
because block traces are simpler.

