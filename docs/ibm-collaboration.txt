==== MINUTES - TRACE2MODEL - IBM COLLABORATION ====

** 24 June 2011 **

This is an initial proposal on the way we can collaborate with IBM on
trace2model project.

Currently, Filebench supports two basic execution abstractions: processes and
threads. To support virtualization emulation, we propose to add two more
abstractions: VMs - virtual machines and HPVs - hyperviser. At the moment, to
define a workload in Filebench, one would write something like that:

define process process1 {
 define thread1 {
   <description of what this thread does>
 }
 define thread2 {
   <description of what this thread does>
 }
 ...
}

define process process2 {
 define thread1 {
   <description of what this thread does>
 }
 define thread2 {
   <description of what this thread does>
 }
 ...
}
...

In the virtualization case, one would define hypervisor and virtual
machines in addition, for example:

define hpv1 {
 define vm1 {
   define process process1 {
     define thread1 {
       <description of what this thread  does>
     }
     define thread1 {
       <description of what this thread  does>
     }
     ...
  }
 ...
 define vm2 {
   define process process1 {
     define thread1 {
       <description of what this thread  does>
     }
     define thread1 {
       <description of what this thread  does>
     }
     ...
  }
 ...
}

define hvp2 {
 <vms with processes with treads defined here, as before>
}

So, when Filebench executes Thread1 operations, it transforms them in according
with VMs and HPVs properties. For example, if Thread1 is defined to writes 1MB
to some file, Filebench can split this write request in a series of 128KB
requests if  VM1 is set up to use IDE controller emulator and HPV1 is VMWare ESX
v.3.1.

In order to understand what transformations happen with I/O requests, we need to
analyse traces from multiple levels. And this is where T2M project comes in: it
should help us to analyse traces. We can focus on this part till FAST
deadline.  After that FSL could work on modifying Filebench to support above
mentioned abstractions. Another modification that we would need to add to
Filebench is NFS-client backend. That's what FSL also could do after the FAST
deadline.

In short, the plan we propose is the following:
1) FSL works on trace analyser
2) IBM provides us with traces from different levels
3) FSL processes these traces using the analyser and makes conclusions:
   3a) validates that the analyser works properly
   3b) looks into how requests transform in a virtualized environment
4) FSL sends generated workload models back to IBM so that IBM can
run them and collect new traces, which FSL can compare to the original
traces for the sake of validity check.

That's the work that we cand do for coming FAST. Filebench modifications and
related work can be postponed to Octorber-December.

** 1 July 2011 **

IBM replied that they like the idea a lot. Dean propsed the following list of
initial items for on-coming discussion:

1) Project goals, responsibilities, paper/FAST

2) Method of sharing traces and tools. Can we setup a wiki and a data
repository?

3) For each trace, need to understand the required trace format into which they
must be converted. Are there any existing tools for converting any of the above
traces? We will definitely need a tool to convert the gpfs traces.

4) Need a way to collect traces from the VM's vfs level (should we just use
strace or can we do something with kprobes? I think you had been working on
something for this).

5) Need to discuss how 'synchronized' the different levels of traces need to be.

6) Need to agree on the levels at which each trace will be taken. Depending on
the level, a different trace type might be used. Currently I see the following
levels:
  a) Guest VFS level (see 1) above)
  b) ESX hypervisor. Do we want just vscsistats tables or actual traces
     (which are still missing the latency information)
  c) File Level: Using GPFS tracing to capture the incoming NFS requests
  d) Block Level: Using GPFS tracing (We store data on a single scsi device to
     make things simple) Note that we use GPFS tracing since it makes it simpler
     to get both the file and block level traces in a single output file.

7) Applications and benchmarks to trace.

Vasily suggests to discuss caching affects at I/O layers as well:

8) How do we emulate caching effects at certain I/O stack levels?
